
@book{zangemeister_nutzwertanalyse_2014,
	location = {Winnemark, Germany},
	edition = {5. Auflage, erweitert},
	title = {Nutzwertanalyse in der Systemtechnik: eine Methodik zur multidimensionalen Bewertung und Auswahl von Projektalternativen},
	isbn = {978-3-923264-00-1},
	shorttitle = {Nutzwertanalyse in der Systemtechnik},
	pagetotal = {414},
	publisher = {Prof. Dr.-Ing. Christof Zangemeister},
	author = {Zangemeister, Christof},
	date = {2014},
	keywords = {(stw)Scoring-Modell, (stw)Systems Engineering, Nutzwertanalyse, Systemtechnik},
	file = {Table of Contents PDF:files/5/Zangemeister - 2014 - Nutzwertanalyse in der Systemtechnik eine Methodi.pdf:application/pdf},
}

@article{rupp_requirements_2009,
	title = {Requirements Engineering und Management},
	volume = {46},
	issn = {2198-2775},
	url = {https://doi.org/10.1007/BF03340367},
	doi = {10.1007/BF03340367},
	abstract = {Requirements Engineering und Management gewinnen in allen Bereichen der Systementwicklung stetig an Bedeutung. Zusammenhänge zwischen der Qualität der Anforderungserhebung und des Projekterfolges, wie von der Standish Group im jährlich erscheinenden Chaos Report [Standish 2004] untersucht, sind den meisten ein Begriff. Bei der Erhebung von Anforderungen treten immer wieder ähnliche Probleme auf. Dabei spielen unterschiedliche Faktoren und Gegebenheiten eine Rolle, die beachtet werden müssen. Es gibt mehrere Möglichkeiten, die Tücken der Analysephase zu meistern; eine Hilfe bietet der Einsatz der in diesem Artikel vorgestellten Methoden zur Anforderungserhebung. Auch wenn die Anforderungen korrekt und vollständig erhoben sind, ist es eine Kunst, diese zu verwalten. In der heutigen Zeit der verteilten Projekte ist es eine Herausforderung, die Dokumentation für jeden Beteiligten ständig verfügbar, nachvollziehbar und eindeutig zu erhalten. Requirements Management rüstet den Analytiker mit Methoden aus, um sich dieser Herausforderung zu stellen. Änderungen von Stakeholder-Wünschen an bestehenden Anforderungen stellen besondere Ansprüche an das Requirements Management, doch mithilfe eines Change-Management-Prozesses können auch diese bewältigt werden. Metriken und Traceability unterstützen bei der Aufwandsabschätzung für Änderungsanträge.},
	pages = {94--103},
	number = {3},
	journaltitle = {{HMD} Praxis der Wirtschaftsinformatik},
	shortjournal = {{HMD}},
	author = {Rupp, Chris and Simon, Matthias and Hocker, Florian},
	urldate = {2021-11-07},
	date = {2009-06-01},
	langid = {german},
	file = {Springer Full Text PDF:files/7/Rupp et al. - 2009 - Requirements Engineering und Management.pdf:application/pdf},
}

@book{kees_open_2015,
	title = {Open source enterprise software},
	publisher = {Springer},
	author = {Kees, Alexandra},
	date = {2015},
	annotation = {Vielleicht eine gute Referenz für das Vorgehen bei einem Marktspiegel.},
	file = {Full Text:files/31/Kees - 2015 - Open source enterprise software.pdf:application/pdf;Snapshot:files/32/10.html:text/html},
}

@article{kreps_kafka_nodate,
	title = {Kafka: a Distributed Messaging System for Log Processing},
	abstract = {Log processing has become a critical component of the data pipeline for consumer internet companies. We introduce Kafka, a distributed messaging system that we developed for collecting and delivering high volumes of log data with low latency. Our system incorporates ideas from existing log aggregators and messaging systems, and is suitable for both offline and online message consumption. We made quite a few unconventional yet practical design choices in Kafka to make our system efficient and scalable. Our experimental results show that Kafka has superior performance when compared to two popular messaging systems. We have been using Kafka in production for some time and it is processing hundreds of gigabytes of new data each day.},
	pages = {7},
	author = {Kreps, Jay and Narkhede, Neha and Rao, Jun},
	langid = {english},
	file = {Kreps et al. - Kafka a Distributed Messaging System for Log Proc.pdf:files/34/Kreps et al. - Kafka a Distributed Messaging System for Log Proc.pdf:application/pdf},
}

@article{g_b_high_2021,
	title = {High Resilient Messaging Service for Microservice Architecture},
	volume = {16},
	issn = {0973-9769, 0973-4562},
	url = {http://ripublication.com/ijaer21/ijaerv16n5_04.pdf},
	doi = {10.37622/IJAER/16.5.2021.357-361},
	abstract = {Fundamental structure of a software system is called software architecture. Software architectures are helpful in systematic software development. One such software architecture is microservice architecture. This architecture breaks down the entire software system into smaller components each of which work independently. The microservice architecture is being widely adapted in software industries due to its reliability, scalability and easier maintenance. But breaking down a system into smaller, independent components that use different tech stacks and message formats gives rise to complex communication between the components. This paper proposes a new messaging service that allows communication between services with ease. The new messaging service uses distributed streaming platforms like Apache Kafka to decouple the messaging between services. It also uses Apache Camel to provide functionalities such as message schema transformation and schema validation; these functionalities allow the services to communicate with each other with only a few lines of code. The result is a new state of the art messaging service that can be easily integrated with producer and consumer services. The paper also discusses where the new messaging service is more suitable. The performance metric used here is the number of additional lines of code required on the producer or consumer side for using this messaging service. The results are, almost thirty percent to sixty percent reduction of code required for integration on the producer side and nine percent to forty percent reduction of code required for integration on the consumer side.},
	pages = {357},
	number = {5},
	journaltitle = {International Journal of Applied Engineering Research},
	author = {G. B., Sanjana and N. S., Girish Rao Salanke},
	urldate = {2021-11-07},
	date = {2021-05-30},
	langid = {english},
	file = {G. B. and N. S. - 2021 - High Resilient Messaging Service for Microservice .pdf:files/36/G. B. and N. S. - 2021 - High Resilient Messaging Service for Microservice .pdf:application/pdf},
}

@article{ranjan_radar-base_2019,
	title = {{RADAR}-Base: Open Source Mobile Health Platform for Collecting, Monitoring, and Analyzing Data Using Sensors, Wearables, and Mobile Devices},
	volume = {7},
	rights = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in {JMIR} {mHealth} and {uHealth}...") is properly cited with original {URL} and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://mhealth.jmir.org/, as well as this copyright and license information must be included.},
	url = {https://mhealth.jmir.org/2019/8/e11734},
	doi = {10.2196/11734},
	shorttitle = {{RADAR}-Base},
	abstract = {Background: With a wide range of use cases in both research and clinical domains, collecting continuous mobile health ({mHealth}) streaming data from multiple sources in a secure, highly scalable, and extensible platform is of high interest to the open source {mHealth} community. The European Union Innovative Medicines Initiative Remote Assessment of Disease and Relapse-Central Nervous System ({RADAR}-{CNS}) program is an exemplary project with the requirements to support the collection of high-resolution data at scale; as such, the Remote Assessment of Disease and Relapse ({RADAR})-base platform is designed to meet these needs and additionally facilitate a new generation of {mHealth} projects in this nascent field. Objective: Wide-bandwidth networks, smartphone penetrance, and wearable sensors offer new possibilities for collecting near-real-time high-resolution datasets from large numbers of participants. The aim of this study was to build a platform that would cater for large-scale data collection for remote monitoring initiatives. Key criteria are around scalability, extensibility, security, and privacy. Methods: {RADAR}-base is developed as a modular application; the backend is built on a backbone of the highly successful Confluent/Apache Kafka framework for streaming data. To facilitate scaling and ease of deployment, we use Docker containers to package the components of the platform. {RADAR}-base provides 2 main mobile apps for data collection, a Passive App and an Active App. Other third-Party Apps and sensors are easily integrated into the platform. Management user interfaces to support data collection and enrolment are also provided. Results: General principles of the platform components and design of {RADAR}-base are presented here, with examples of the types of data currently being collected from devices used in {RADAR}-{CNS} projects: Multiple Sclerosis, Epilepsy, and Depression cohorts. Conclusions: {RADAR}-base is a fully functional, remote data collection platform built around Confluent/Apache Kafka and provides off-the-shelf components for projects interested in collecting {mHealth} datasets at scale.},
	pages = {e11734},
	number = {8},
	journaltitle = {{JMIR} {mHealth} and {uHealth}},
	author = {Ranjan, Yatharth and Rashid, Zulqarnain and Stewart, Callum and Conde, Pauline and Begale, Mark and Verbeeck, Denny and Boettcher, Sebastian and Hyve, The and Dobson, Richard and Folarin, Amos and Consortium, The {RADAR}-{CNS}},
	urldate = {2021-11-07},
	date = {2019-08-01},
	note = {Company: {JMIR} {mHealth} and {uHealth}
Distributor: {JMIR} {mHealth} and {uHealth}
Institution: {JMIR} {mHealth} and {uHealth}
Label: {JMIR} {mHealth} and {uHealth}
Publisher: {JMIR} Publications Inc., Toronto, Canada},
	file = {Full Text:files/39/Ranjan et al. - 2019 - RADAR-Base Open Source Mobile Health Platform for.pdf:application/pdf;Snapshot:files/40/e11734.html:text/html},
}

@article{muruts_multi-tenant_nodate,
	title = {Multi-Tenant Apache Kafka for Hops},
	abstract = {Apache Kafka is a distributed, high throughput and fault-tolerant publish/subscribe messaging system in the Hadoop ecosystem. It is used as a distributed data streaming and processing platform. Kafka topics are the units of message feeds in the Kafka cluster. Kafka producer publishes messages into these topics and a Kafka consumer subscribes to topics to pull those messages. With the increased usage of Kafka in the data infrastructure of many companies, there are many Kafka clients that publish and consume messages to/from the Kafka topics. In fact, these client operations can be malicious. To mitigate this risk, clients must authenticate themselves and their operation must be authorized before they can access to a given topic. Nowadays, Kafka ships with a pluggable Authorizer interface to implement access control list ({ACL}) based authorization for client operation. Kafka users can implement the interface differently to satisfy their security requirements. {SimpleACLAuthorizer} is the out-of-box implementation of the interface and uses a Zookeeper for {ACLs} storage. {HopsWorks}, based on Hops - a next generation Hadoop distribution, provides support for project-based multi-tenancy, where projects are fully isolated at the level of the Hadoop Filesystem and {YARN}. In this project, we added Kafka topicbased multi-tenancy in Hops projects. Kafka topic is created from inside Hops project and persisted both at the Zookeeper and the {NDBCluster}. Persisting a topic into a database enabled us for topic sharing across projects. {ACLs} are added to Kafka topics and are persisted only into the database. Client access to Kafka topics is authorized based on these {ACLs}. {ACLs} are added, updated, listed and/or removed from the {HopsWorks} {WebUI}. {HopsACLAuthorizer}, a Hops implementation of the Authorizer interface, authorizes Kafka client operations using the {ACLs} in the database. The Apache Avro schema registry for topics enabled the producer and consumer to better integrate by transferring a preestablished message format. The result of this project is the ﬁrst Hadoop distribution that supports Kafka multi-tenancy.},
	pages = {70},
	author = {Muruts, Misganu Dessalegn},
	langid = {english},
	file = {Muruts - Multi-Tenant Apache Kafka for Hops.pdf:files/41/Muruts - Multi-Tenant Apache Kafka for Hops.pdf:application/pdf},
}

@article{auer_distributed_nodate,
	title = {Distributed Data Store for Internet of Things Environments},
	pages = {62},
	author = {Auer, Jonas},
	langid = {english},
	file = {Auer - Distributed Data Store for Internet of Things Envi.pdf:files/43/Auer - Distributed Data Store for Internet of Things Envi.pdf:application/pdf},
}

@article{korhonen_using_nodate,
	title = {Using Kafka to Build Scalable and Fault Tolerant Systems},
	abstract = {During the development phase the project was thoroughly tested through a large number of {JUnit} tests which allowed for multiple bugs to be eliminated and resulted in a better understanding of how Kafka functions at a deeper level. The end result of the study was a fully functioning messaging queue implemented into the Java project through the use of Apache Kafka.},
	pages = {26},
	author = {Korhonen, Teemu},
	langid = {english},
	file = {Korhonen - Using Kafka to Build Scalable and Fault Tolerant S.pdf:files/45/Korhonen - Using Kafka to Build Scalable and Fault Tolerant S.pdf:application/pdf},
}

@article{muller_iot_nodate,
	title = {{IoT} for All: Architectural Design of an Extensible and Lightweight {IoT} Analytics Platform},
	abstract = {Realizing Internet of Things ({IoT}) is still a huge challenge for Small and Medium-sized Enterprises ({SMEs}) caused by heavy upfront capital investments and inadequate infrastructure. An opportunity to counteract the concern is provided in this paper by introducing an extensible and lightweight {IoT} analytics platform. Therefore, we present an architectural design offering the possibility of connecting arbitrary {IoT} devices to the analytics platform which includes data processing and analytics, data storage as well as data delivery. Regardless of the integration of already purchased non {IoT}-ready devices or acquisition of new {IoT} capable devices, the design allows to gradually build an {IoT} environment in enterprises provoked by its modular structure and relying on only open source software-based tools.},
	pages = {6},
	journaltitle = {Data Science},
	author = {Müller, Stephan and Wiener, Patrick and Bürger, Adrian and Nimis, Jens},
	langid = {english},
	file = {Müller et al. - IoT for All Architectural Design of an Extensible.pdf:files/47/Müller et al. - IoT for All Architectural Design of an Extensible.pdf:application/pdf},
}

@article{jephte_extract_nodate,
	title = {Extract, Transform, and Load data from Legacy Systems to Azure Cloud},
	pages = {66},
	author = {Jephte, Ioudom Foubi},
	langid = {english},
	file = {Jephte - Extract, Transform, and Load data from Legacy Syst.pdf:files/49/Jephte - Extract, Transform, and Load data from Legacy Syst.pdf:application/pdf},
}

@article{holom_metadata_2020,
	title = {Metadata management in a big data infrastructure},
	volume = {42},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978920306247},
	doi = {10.1016/j.promfg.2020.02.060},
	series = {International Conference on Industry 4.0 and Smart Manufacturing ({ISM} 2019)},
	abstract = {The adoption of the Internet of Things ({IoT}) in industry provides the opportunity to gather valuable data. Nevertheless, this amount of data must be analyzed to identify patterns in the data, model behaviors of equipment and to enable prediction. Although big data found its initiation already some years ago, there are still many challenges to be solved, e.g. metadata representation and management are still a research topic. The big data architecture of the {RISC} data analytics framework relies on the combination of big data technologies with semantic approaches, to process and store large volumes of data from heterogeneous sources, provided by {FILL}, which is a key machine tool provider. The proposed architecture is capable of handling sensor data using big data technologies such as Spark on Hadoop, {InfluxDB} and Elasticsearch. The metadata representation and management approach is adopted in order to define the structure and the relations (i.e., the connections) between the various data sources provided by the sensors and logging information system. On the other hand, using a metadata approach in our big data environment enhances {RISC} data analytics framework by making it generic, reusable and responsive in case of changes, thus keeping the data lakes up-to-date and ensuring the validity of the analytics results. The work presented here is part of an ongoing project ({BOOST} 4.0) currently addressed under the {EU} H2020 program.},
	pages = {375--382},
	journaltitle = {Procedia Manufacturing},
	shortjournal = {Procedia Manufacturing},
	author = {Holom, Roxana-Maria and Rafetseder, Katharina and Kritzinger, Stefanie and Sehrschön, Harald},
	urldate = {2021-11-07},
	date = {2020-01-01},
	langid = {english},
	keywords = {big data, data harmonization, linked data, machine learning, metadata, semantics},
	file = {ScienceDirect Full Text PDF:files/52/Holom et al. - 2020 - Metadata management in a big data infrastructure.pdf:application/pdf},
}